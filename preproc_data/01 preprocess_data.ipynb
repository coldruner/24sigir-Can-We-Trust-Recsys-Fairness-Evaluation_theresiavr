{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it works:\n",
    "- Execute run.py to get the initial train, val, test splits (saved in the train_val_test folder)\n",
    "- Move the files to the folder train_val_test_before_remove_train\n",
    "- This notebook makes use of those splits and preprocess them further.\n",
    "- We save the further preprocessed datasets as ''benchmark files'' for input to RecBole. Those will be our final datasets that we will use for training, validation, and test.\n",
    "- Execute run_new.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_df(df, dataset_name):\n",
    "    print(\"Getting stats from these columns: \", df.columns[0:2])\n",
    "    num_user = df.iloc[:,0].unique().shape[0]\n",
    "    num_item = df.iloc[:,1].unique().shape[0]\n",
    "    num_inter = len(df)\n",
    "    sparsity = 1 - num_inter / (num_user * num_item)\n",
    "    print(\"Statistics: \")\n",
    "    print(f\"Number of users: {num_user}\")\n",
    "    print(f\"Number of items: {num_item}\")\n",
    "    print(f\"Number of interactions: {num_inter}\")\n",
    "    print(f\"Sparsity: {sparsity}\")\n",
    "\n",
    "    return {dataset_name: [num_user, num_item, num_inter, sparsity]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "    print(f\"Loading {dataset_name}\")\n",
    "    df = pd.read_csv(f\"../dataset/{dataset_name}/{dataset_name}.inter\", sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "def load_preprocessed_data(dataset, path=\"train_val_test\"):\n",
    "\n",
    "    with open(f\"../{path}/{dataset}_train.pickle\",\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    train = pd.DataFrame(data)\n",
    "\n",
    "    with open(f\"../{path}/{dataset}_valid.pickle\",\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    val = pd.DataFrame(data)\n",
    "\n",
    "    with open(f\"../{path}/{dataset}_test.pickle\",\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    test = pd.DataFrame(data)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "def concat(train, val, test):\n",
    "    return pd.concat([train, val, test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataset = [\"Amazon-lb\",\"Lastfm\",\"ML-10M\", \"QK-video\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def further_preprocess(dataset_name):\n",
    "\n",
    "        train, val, test = load_preprocessed_data(dataset_name, \"train_val_test_before_remove_train\")\n",
    "\n",
    "        #Remove users in train with less than 5 interactions (only keep those with at least 5)\n",
    "\n",
    "        new_train_agg = train\\\n",
    "                .groupby(\"user_id\")\\\n",
    "                .count()\n",
    "        new_train_id = new_train_agg[new_train_agg.iloc[:, 0]>=5].index\n",
    "        new_train = train[train.user_id.isin(new_train_id)]\n",
    "\n",
    "        #Completely remove those users in val and test\n",
    "        new_val = val[val.user_id.isin(new_train_id)]\n",
    "        new_test =  test[test.user_id.isin(new_train_id)]\n",
    "\n",
    "        #Ensure all val and test users are in new train\n",
    "        assert new_val.user_id.isin(new_train_id).all()\n",
    "        assert new_test.user_id.isin(new_train_id).all()\n",
    "\n",
    "        #ensure each user in train has at least 5\n",
    "        assert all(new_train\\\n",
    "                .groupby(\"user_id\")\\\n",
    "                .count()\\\n",
    "                .iloc[:, 0] >= 5)\n",
    "\n",
    "        return new_train, new_val, new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_inter(df:pd.DataFrame, col_name_dict:dict, file_name:str, dataset_name):\n",
    "    inter = df.copy()\n",
    "    inter.rename(columns=col_name_dict, inplace=True)\n",
    "\n",
    "    path = f\"../preproc_data/new_{dataset_name}/\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    inter.to_csv(path+file_name, index=False, sep=\"\\t\")\n",
    "    return inter\n",
    "\n",
    "def create_file(dataset_name):\n",
    "    train, val, test = further_preprocess(dataset_name)\n",
    "\n",
    "    col_name_dict = {\n",
    "                \"user_id\":\"user_id:token\",\n",
    "                \"item_id\":\"item_id:token\",\n",
    "                \"artist_id\":\"artist_id:token\",\n",
    "                \"label\":\"label:float\",\n",
    "                \"timestamp\":\"timestamp:float\"\n",
    "                }\n",
    "    \n",
    "    #this method converts our loaded dataframe to a .inter file, and saves it in the folder data under the name 'file_name'\n",
    "    convert_df_to_inter(train, col_name_dict, f\"new_{dataset_name}.train.inter\", dataset_name)\n",
    "    convert_df_to_inter(val, col_name_dict,f\"new_{dataset_name}.valid.inter\", dataset_name)\n",
    "    convert_df_to_inter(test, col_name_dict, f\"new_{dataset_name}.test.inter\", dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented to avoid accidental run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in list_dataset:\n",
    "#     create_file(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Stats\n",
    "Count number of user, item, interaction.\n",
    "\n",
    "This can be run after the instruction at the beginning of the notebook has been done (including run_new.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataset = [\"Amazon-lb\",\"Lastfm\",\"ML-10M\",\"QK-video\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "old_preproc_result = {}\n",
    "preproc_result = {}\n",
    "train_val_test_result = {}\n",
    "test_df = {}\n",
    "\n",
    "for data in list_dataset:\n",
    "    df = load_data(data)\n",
    "    stat_data = stat_df(df, data)\n",
    "    result.update(stat_data)\n",
    "\n",
    "    #old\n",
    "    old_train, old_val, old_test = load_preprocessed_data(data, \"train_val_test_before_remove_train\")\n",
    "    old_preproc_data = concat(old_train,old_val,old_test)\n",
    "\n",
    "    old_preproc_stat_data = stat_df(old_preproc_data, data)\n",
    "    old_preproc_result.update(old_preproc_stat_data)\n",
    "\n",
    "    #new\n",
    "    train, val, test = load_preprocessed_data(\"new_\"+data,  \"train_val_test\")\n",
    "    preproc_data = concat(train,val,test)\n",
    "\n",
    "    preproc_stat_data = stat_df(preproc_data, \"new_\"+data)\n",
    "    preproc_result.update(preproc_stat_data)\n",
    "    test_df[data] = test\n",
    "\n",
    "    #per split\n",
    "    for i, (old_split, new_split) in enumerate(zip([old_train, old_val, old_test],[train, val, test])):\n",
    "        old_stat = stat_df(old_split, data)\n",
    "        new_stat = stat_df(new_split, \"new_\"+data)\n",
    "        if i not in train_val_test_result:\n",
    "            train_val_test_result[i] = old_stat\n",
    "            train_val_test_result[i].update(new_stat)\n",
    "        else:\n",
    "            train_val_test_result[i].update(old_stat)\n",
    "            train_val_test_result[i].update(new_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(result).T\n",
    "df_result.columns = [\"num_user\", \"num_item\", \"num_inter\", \"sparsity\"]\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.sort_values(\"num_inter\").to_excel(\"stats/dataset_statistics.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_index = df_result.sort_values(\"num_inter\").index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_result = pd.DataFrame(preproc_result).T\n",
    "preproc_result.columns = [\"num_user\", \"num_item\", \"num_inter\", \"sparsity\"]\n",
    "preproc_result.loc[\"new_\"+sorted_index].to_excel(\"stats/new_dataset_statistics_preprocessed.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
